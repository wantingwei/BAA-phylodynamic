{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b91a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is to perform the sequence clustering, to use this code\n",
    "#1. prepare nextcalde clade info and aligned fasta file, can use nextclade CLI to do this easily\n",
    "#2. install necessary package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b25be16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9h/h1kjxm6j26ggb2qml3vbbxg80000gs/T/ipykernel_84775/4274116231.py:5: DtypeWarning: Columns (28,50,51,52,53,54,55,56,57,58,59,61,75,79,81,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(metadata_file, sep =\"\\t\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the metadata file\n",
    "metadata_file = \"output_04_14/nextclade_04_14.tsv\"\n",
    "df = pd.read_csv(metadata_file, sep =\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9219dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences dropped (unique lineages): 293\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Drop lineages that occur only once, please remember to add this number for your signletons cluster\n",
    "original_count = len(df)\n",
    "df_filtered = df[df.groupby(\"Nextclade_pango\")[\"Nextclade_pango\"].transform(\"count\") > 1]\n",
    "filtered_count = len(df_filtered)\n",
    "dropped_count = original_count - filtered_count\n",
    "print(f\"Number of sequences dropped (unique lineages): {dropped_count}\")\n",
    "\n",
    "# Step 3: Group by Lineage and extract sequence strain\n",
    "lineage_to_seqnames = (\n",
    "    df_filtered.groupby(\"Nextclade_pango\")[\"seqName\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Group sequenes from the aligned fasta file to individual file, I highly recommend using this approach as it is much easiler to run small file each time compare to super large file\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "\n",
    "input_fasta = \"04_14_nextclade.aligned.fasta\"  # replace with actual path\n",
    "\n",
    "# Parse once, and store records by name for fast lookup\n",
    "all_records = SeqIO.to_dict(SeqIO.parse(input_fasta, \"fasta\"))\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(\"lineage_group_fasta\", exist_ok=True)\n",
    "\n",
    "# Then, write out grouped FASTAs\n",
    "for lineage, names in lineage_to_seqnames.items():\n",
    "    output_file = f\"lineage_group_fasta/{lineage}_group.fasta\"\n",
    "    \n",
    "    # Identify missing names\n",
    "    missing_names = [name for name in names if name not in all_records]\n",
    "    if missing_names:\n",
    "        print(f\"Warning: {len(missing_names)} sequences not found for lineage {lineage}\")\n",
    "        # Optionally, print the missing sequence names\n",
    "        # print(f\"Missing: {missing_names}\")\n",
    "\n",
    "    # Write only found sequences\n",
    "    selected_records = [all_records[name] for name in names if name in all_records]\n",
    "    \n",
    "    if selected_records:\n",
    "        with open(output_file, \"w\") as out_f:\n",
    "            SeqIO.write(selected_records, out_f, \"fasta\")\n",
    "    else:\n",
    "        print(f\"Warning: No sequences written for lineage {lineage} ‚Äî all sequence names missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5db6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, use pairsnp to caculate the distance matric of a given clade\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pairsnp import calculate_snp_matrix, calculate_distance_matrix\n",
    "\n",
    "base_dir = \"MN_data/lineage_group_fasta/\"\n",
    "test_one = True  # Set True to test just one group\n",
    "\n",
    "# Track which clades are empty\n",
    "empty_clades = []\n",
    "\n",
    "# Loop through each grouped FASTA file\n",
    "for filename in sorted(os.listdir(base_dir)):\n",
    "    if filename.endswith(\"_group.fasta\"):\n",
    "        fasta_path = os.path.join(base_dir, filename)\n",
    "        lineage_name = filename.replace(\"_group.fasta\", \"\")\n",
    "\n",
    "        try:\n",
    "            print(f\"Processing: {fasta_path}\")\n",
    "\n",
    "            # Run PairSNP analysis\n",
    "            sparse_matrix, consensus, seq_names = calculate_snp_matrix(fasta_path)\n",
    "            d = calculate_distance_matrix(sparse_matrix, consensus, \"dist\", False)\n",
    "\n",
    "            # Save labeled distance matrix\n",
    "            df = pd.DataFrame(d, index=seq_names, columns=seq_names)\n",
    "            output_csv = os.path.join(base_dir, f\"distance_matrix_group/{lineage_name}_snp_distance_matrix.csv\")\n",
    "            df.to_csv(output_csv)\n",
    "\n",
    "        except ValueError as e:\n",
    "            if \"No sequences found\" in str(e):\n",
    "                print(f\"‚ö†Ô∏è Skipping {lineage_name} ‚Äî no sequences found.\")\n",
    "                empty_clades.append(lineage_name)\n",
    "            else:\n",
    "                raise  # Re-raise other unexpected errors\n",
    "\n",
    "# Save the list of empty clades\n",
    "if empty_clades:\n",
    "    log_path = os.path.join(base_dir, \"empty_clades_log.txt\")\n",
    "    with open(log_path, \"w\") as log_file:\n",
    "        for clade in empty_clades:\n",
    "            log_file.write(f\"{clade}\\n\")\n",
    "\n",
    "    print(f\"\\nüìù Empty clades saved to: {log_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after caculate the distance matrix, now lets find the identical sequence, the code here is for identicall sequence, you can change the distance to include more seq\n",
    "# the code is validated on 04-15 fix the duplicated problem\n",
    "#the code will generate two file, one include the cluster summary, one include the indiviudal seq within a given cluster\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "import numpy as np\n",
    "\n",
    "# === Set input directory containing *_snp_distance_matrix.csv files ===\n",
    "input_dir = \"MN_data/lineage_group_fasta/distance_matrix_group/\"\n",
    "\n",
    "# === Containers for outputs ===\n",
    "cluster_summary = []\n",
    "detailed_clusters = []\n",
    "\n",
    "def find_clusters(distance_df):\n",
    "    \"\"\"Return list of clusters (sets) using graph connectivity (distance = 0)\"\"\"\n",
    "    matrix = (distance_df.values == 0).astype(int)\n",
    "    np.fill_diagonal(matrix, 0)  # Ignore self-distances\n",
    "    graph = csr_matrix(matrix)\n",
    "\n",
    "    # Compute connected components\n",
    "    n_components, labels = connected_components(csgraph=graph, directed=False)\n",
    "    clusters = defaultdict(set)\n",
    "\n",
    "    for seq, label in zip(distance_df.index, labels):\n",
    "        clusters[label].add(seq)\n",
    "\n",
    "    return list(clusters.values())\n",
    "\n",
    "\n",
    "# === Loop through files ===\n",
    "for file in sorted(os.listdir(input_dir)):\n",
    "    if not file.endswith(\"_snp_distance_matrix.csv\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(input_dir, file)\n",
    "    lineage = file.replace(\"_snp_distance_matrix.csv\", \"\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "        # Find clusters of identical sequences\n",
    "        clusters = find_clusters(df)\n",
    "\n",
    "        # Count clusters by size\n",
    "        size_count = defaultdict(int)\n",
    "        for cluster in clusters:\n",
    "            size_count[len(cluster)] += 1\n",
    "\n",
    "        # Add summary\n",
    "        for size, count in size_count.items():\n",
    "            cluster_summary.append({\n",
    "                \"Lineage\": lineage,\n",
    "                \"Cluster_Size\": size,\n",
    "                \"Count\": count\n",
    "            })\n",
    "\n",
    "        # Add detailed members\n",
    "        for i, cluster in enumerate(clusters, 1):\n",
    "            detailed_clusters.append({\n",
    "                \"Lineage\": lineage,\n",
    "                \"Cluster_ID\": f\"{lineage}_Cluster_{i}\",\n",
    "                \"Members\": \";\".join(sorted(cluster))\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {file}: {e}\")\n",
    "\n",
    "# === Convert to DataFrames ===\n",
    "summary_df = pd.DataFrame(cluster_summary)\n",
    "detailed_df = pd.DataFrame(detailed_clusters)\n",
    "\n",
    "# === Save results ===\n",
    "summary_df.to_csv(os.path.join(input_dir, \"all_lineage_cluster_summary.csv\"), index=False)\n",
    "detailed_df.to_csv(os.path.join(input_dir, \"all_lineage_cluster_membership.csv\"), index=False)\n",
    "\n",
    "print(\"‚úÖ Finished. Summary and detailed cluster membership saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb9a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#additonal code for count number of each cluster\n",
    "import pandas as pd\n",
    "\n",
    "# === Load the cluster summary file ===\n",
    "summary_path = \"MN_data/lineage_group_fasta/distance_matrix_group/all_lineage_cluster_summary.csv\"  # Update if needed\n",
    "summary_df = pd.read_csv(summary_path)\n",
    "\n",
    "# === Group by Cluster_Size and sum the counts ===\n",
    "size_summary = summary_df.groupby(\"Cluster_Size\")[\"Count\"].sum().reset_index()\n",
    "\n",
    "# === Save detailed counts for all cluster sizes ===\n",
    "size_summary.to_csv(\"MN_data/lineage_group_fasta/distance_matrix_group/cluster_size_summary_04_15.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Saved detailed cluster size summary to 'cluster_size_summary.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf542eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to extract metadata to cluster from the large metadata file\n",
    "# the code is slighlty redunant as the the seqName contains strain name from different source,but the point is to extract all the associated metadata of a given cluster for RR analysis\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# === Load Data ===\n",
    "membership_df = pd.read_csv(\"MN_data/lineage_group_fasta/distance_matrix_group/all_lineage_cluster_membership.csv\")\n",
    "metadata_df = pd.read_csv(\"MN_data/matched_metadata_final_drop_doplicate.csv\")\n",
    "\n",
    "# === Output folder ===\n",
    "output_dir = \"cluster_metadata_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Helper function ===\n",
    "def normalize_member_ids(members_raw):\n",
    "    members_fixed = []\n",
    "    for m in members_raw:\n",
    "        match = re.search(r'hCoV-19/USA/([A-Z]{2}-[^/]+)', m)\n",
    "        if match:\n",
    "            extracted = match.group(1)\n",
    "            print(f\"‚úÖ Match: {m} ‚Üí {extracted}\")\n",
    "            members_fixed.append(extracted)\n",
    "        else:\n",
    "            print(f\"‚ùå Failed match: {m}\")\n",
    "            members_fixed.append(None)\n",
    "    return members_fixed\n",
    "\n",
    "# === Skip clusters with only 1 member ===\n",
    "membership_df[\"Member_Count\"] = membership_df[\"Members\"].apply(lambda x: len(x.split(\";\")))\n",
    "\n",
    "for idx, row in membership_df.iterrows():\n",
    "    cluster_id = row[\"Cluster_ID\"]\n",
    "    members_raw = row[\"Members\"].split(\";\")\n",
    "\n",
    "    if len(members_raw) <= 1:\n",
    "        continue  # ‚úÖ skip singleton clusters\n",
    "\n",
    "    # Normalize member names\n",
    "    members_fixed = normalize_member_ids(members_raw)\n",
    "\n",
    "    if None in members_fixed:\n",
    "        raise ValueError(f\"‚ùå Cluster '{cluster_id}' has unrecognized member format: {members_raw}\")\n",
    "\n",
    "    # Match both dash and underscore forms\n",
    "    members_fixed_all = set(members_fixed + [m.replace(\"-\", \"_\") for m in members_fixed if m])\n",
    "    metadata_matches = metadata_df[metadata_df[\"normalized_strain\"].isin(members_fixed_all)].copy()\n",
    "    metadata_matches[\"Cluster_ID\"] = cluster_id\n",
    "\n",
    "    # Raise error if any are missing\n",
    "    matched_set = set(metadata_matches[\"normalized_strain\"])\n",
    "    missing = [m for m in members_fixed if m and m not in matched_set and m.replace(\"-\", \"_\") not in matched_set]\n",
    "    if missing:\n",
    "        raise ValueError(f\"‚ùå Cluster '{cluster_id}' has missing members: {missing}\")\n",
    "\n",
    "    # Save metadata for this cluster\n",
    "    output_path = os.path.join(output_dir, f\"{cluster_id}_metadata.tsv\")\n",
    "    metadata_matches.to_csv(output_path, sep='\\t', index=False)\n",
    "    print(f\"‚úÖ Saved: {cluster_id} ({len(metadata_matches)} records)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e16366bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9h/h1kjxm6j26ggb2qml3vbbxg80000gs/T/ipykernel_8813/4142577797.py:5: DtypeWarning: Columns (24,25,27,28,30,31,32,33,35,36,37,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata_df = pd.read_csv(\"2025-04-03_matched_MN_meta_toCHECK_v2.tsv\", sep=\"\\t\")\n",
      "/var/folders/9h/h1kjxm6j26ggb2qml3vbbxg80000gs/T/ipykernel_8813/4142577797.py:6: DtypeWarning: Columns (28,50,51,52,53,54,55,56,57,58,59,61,75,79,81,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nextclade_df = pd.read_csv(\"output_04_14/nextclade_04_14.tsv\", sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matched metadata rows: 82947\n"
     ]
    }
   ],
   "source": [
    "#code for only keeping metadata row in the fastafile\n",
    "import pandas as pd\n",
    "\n",
    "# Load input files\n",
    "metadata_df = pd.read_csv(\"2025-04-03_matched_MN_meta_toCHECK_v2.tsv\", sep=\"\\t\")\n",
    "nextclade_df = pd.read_csv(\"output_04_14/nextclade_04_14.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Step 1: Extract core ID from nextclade seqName (e.g., hCoV-19/USA/MN-CDC-XXX/2021)\n",
    "nextclade_df['core_id'] = nextclade_df['seqName'].str.extract(r'hCoV-19/USA/([^/]+)', expand=False)\n",
    "\n",
    "# Step 2: Replace underscores with dashes for consistency\n",
    "nextclade_df['core_id'] = nextclade_df['core_id'].str.replace('_', '-', regex=False)\n",
    "metadata_df['normalized_strain'] = metadata_df['strain'].str.replace('_', '-', regex=False)\n",
    "\n",
    "# Step 3: Filter metadata to keep only matched strains\n",
    "matched_metadata_df = metadata_df[metadata_df['normalized_strain'].isin(nextclade_df['core_id'])]\n",
    "\n",
    "# Step 4: Save the matched metadata to a new file\n",
    "matched_metadata_df.to_csv(\"matched_metadata_final.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Step 5: Report number of matches\n",
    "print(f\"Number of matched metadata rows: {matched_metadata_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca4331d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c163e8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9h/h1kjxm6j26ggb2qml3vbbxg80000gs/T/ipykernel_8813/3735541180.py:4: DtypeWarning: Columns (24,25,27,28,30,31,32,33,35,36,37,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata_df = pd.read_csv(\"2025-04-03_matched_MN_meta_toCHECK_v2.tsv\", sep=\"\\t\")\n",
      "/var/folders/9h/h1kjxm6j26ggb2qml3vbbxg80000gs/T/ipykernel_8813/3735541180.py:5: DtypeWarning: Columns (28,50,51,52,53,54,55,56,57,58,59,61,75,79,81,85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nextclade_df = pd.read_csv(\"output_04_14/nextclade_04_14.tsv\", sep=\"\\t\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matched sequences: 82946\n"
     ]
    }
   ],
   "source": [
    "#checking for which row is present in the metadata\n",
    "\n",
    "# Load metadata and Nextclade files\n",
    "metadata_df = pd.read_csv(\"2025-04-03_matched_MN_meta_toCHECK_v2.tsv\", sep=\"\\t\")\n",
    "nextclade_df = pd.read_csv(\"output_04_14/nextclade_04_14.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Extract core ID from both files (e.g., MN-MDH-32441 or MN_UMGC_28146)\n",
    "nextclade_df['core_id'] = nextclade_df['seqName'].str.extract(r'(MN[-_][A-Z]+[-_]\\d+)')\n",
    "metadata_df['core_id'] = metadata_df['covid_virus_name'].str.extract(r'(MN[-_][A-Z]+[-_]\\d+)')\n",
    "\n",
    "# Normalize both core_id columns by replacing underscores with dashes\n",
    "nextclade_df['core_id'] = nextclade_df['core_id'].str.replace('_', '-', regex=False)\n",
    "metadata_df['core_id'] = metadata_df['core_id'].str.replace('_', '-', regex=False)\n",
    "\n",
    "# Filter to matched entries\n",
    "matched_df = nextclade_df[nextclade_df['core_id'].isin(metadata_df['core_id'])]\n",
    "\n",
    "# Print total number of matched entries\n",
    "print(\"Number of matched sequences:\", matched_df.shape[0])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
